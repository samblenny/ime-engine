#!/usr/bin/ruby
# coding: utf-8
require 'erb'
require 'set'

RUST_FILE = "../src/autogen_hsk.rs"
WORD_FILES = [
  "hsk1.tsv",
  "hsk1-extra.tsv",
  "hsk2.tsv",
  "hsk2-extra.tsv",
  "hsk3.tsv",
]

# Returns array: [[hanzi, pinyin], [hanzi, pinyin], ...]
# The `select {..."#"..."\t"}` filters out blank lines and comments
def read_tsv(file)
  File.read(file).lines
    .select { |n| !n.start_with?("#") && n.include?("\t") }
    .map { |n| n.chomp.split("\t") }
end

# Make a set with each unique character used in pinyin from <file>.
def char_set(file)
  Set.new(read_tsv(file).map { |_, pinyin| pinyin.downcase.chars }.flatten)
end

# Normalize pinyin to lowercase ASCII (remove diacritics/whitespace/punctuation).
TR_FROM = " 'abcdefghijklmnopqrstuwxyzàáèéìíòóùúüāēěīōūǎǐǒǔǚ"
TR_TO   = " 'abcdefghijklmnopqrstuwxyzaaeeiioouuvaeeiouaiouv"
ELIDE   = " '"
def normalize(pinyin)
  n = pinyin.downcase.delete(ELIDE).tr(TR_FROM, TR_TO)
  abort "Error: normalize(#{pinyin}) gave #{n} (non-ascii). Check TR_FROM & TR_TO." if !n.ascii_only?
  return n
end

# Check integrity and coverage of the character transposition table.
# The map/reduce uses set algebra to build a sorted string of unique characters from all the files.
detected = WORD_FILES.map {|wf| char_set(wf)}.reduce {|a,b| a+b}.to_a.sort.join("")
if detected != TR_FROM
  warn "Error: Characters used in word file pinyin do not match TR_FROM"
  warn " detected: \"#{detected}\""
  warn " TR_FROM:  \"#{TR_FROM}\""
  abort "You need to update TR_FROM and TR_TO so pinyin will properly normalized to ASCII"
end
abort "Error: Check for TR_FROM/TR_TO length mismatch" if TR_FROM.size != TR_TO.size

# Merge hanzi values for duplicate pinyin search keys
# example: ["he", "he"] and ["喝", "和"] get turned into ["he"] and ["喝\t和"]
merged_hanzi = []
merged_pinyin = []
first_index_of = {}
duplicate_pinyin = []
i = 0
for wf in WORD_FILES
  for hanzi, pinyin in read_tsv(wf)
    normalized_pinyin = normalize(pinyin)
    if first_index_of[normalized_pinyin]
      if merged_hanzi[first_index_of[normalized_pinyin]].include?(hanzi)
        # If you see this warning, check for two files listing the exact same word
        warn "Likely duplicate word: #{wf}:  #{hanzi}:#{pinyin}  ==>  try:  grep #{hanzi} *.tsv"
      end
      # Conditionally append hanzi for duplicate search key
      # 1. Skip hanzi like 过 guò with same hanzi, same pinyin, but different part of speech
      # 2. For the rest, append the new hanzi to the list of choices
      if !merged_hanzi[first_index_of[normalized_pinyin]].include?(hanzi)
        merged_hanzi[first_index_of[normalized_pinyin]] << hanzi
        duplicate_pinyin << normalized_pinyin
      else
        warn "SKIPPING insert of duplicate (hanzi,pinyin) from #{wf}:  #{hanzi} #{normalized_pinyin}"
      end
    else
      # First instance of search key ==> Add new entries
      merged_hanzi[i] = [hanzi]
      merged_pinyin[i] = normalized_pinyin
      first_index_of[normalized_pinyin] = i
      i += 1
    end
  end
end

# Sort the merged vocab lists in pinyin order
merged_pinyin, merged_hanzi = merged_pinyin.zip(merged_hanzi).sort.transpose
duplicate_pinyin = duplicate_pinyin.sort.uniq

puts "\nPreparing to generate rust source code..."
print "This will overwrite #{RUST_FILE}\nDo you want to continue? [y/N] "
abort "no changes made" if !["y", "Y"].include? gets.chomp

# Generate rust source code with hanzi and pinyin arrays
File.open(RUST_FILE, "w") { |rf|
  TEMPLATE = <<~RUST
    // This file is automatically generated. DO NOT MAKE EDITS HERE!
    // To make changes, see ../vocab/autogen-hsk.rb

    // The hanzi values for these duplicate pinyin search keys were merged:
    <% duplicate_pinyin.each do |dp| %>//  <%= dp %>
    <% end %>
    pub const HANZI: &[&'static str] = &[
    <% merged_hanzi.each do |h| %>    &"<%= h.join("\t") %>",
    <% end %>];

    pub const PINYIN: &[&'static str] = &[
    <% merged_pinyin.each do |p| %>    &"<%= p %>",
    <% end %>];
    RUST
  rf.puts ERB.new(TEMPLATE).result(binding)
}
